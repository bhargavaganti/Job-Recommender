{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//media/patrick/MY_EXTERNAL/git/Fletcher\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('jobs_descriptions_all.pk', 'rb') as handle:\n",
    "    jobs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3374"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "jobsy = TextBlob(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'careers data scientist business intelligence', u'analytics austin tx', u'key member', u'data science team', u'customer data', u'statistical models', u'digital experience', u'scientific rigor', u'accurate predictive models', u'customer behavior', u'strong analytical ability', u'drive key success metrics', u'revenue generation', u'big data', u'ideal candidate', u'statistics background', u'direct response', u'analysis metrics', u'customer acquisition retention', u'quantitative analysis', u'computer intensive data', u'techniques decision trees neural networks', u'actionable insights', u'users database experience', u'extract data', u'various sources', u'general guidance design construct', u'complex mathematical models', u'ideal candidate', u'masters degree', u'mathematics statistics', u'quantitative discipline', u'operations research', u'economics proficiency', u'r python sas', u'spss ms excel sql', u'database analytics tools', u'extract analyze', u'large data warehouses familiarity', u'google analytics bigquery hadoop tableau web analytics', u'granular data', u'hadoop data storage platform exceptional analytical skills', u'strong background', u'quantitative analysis', u'logistic regression decision trees', u'machine learning techniques', u'principles theories concepts', u'uncertainty projects assignments', u'great medical dental plans', u'competitive salary target', u'annual bonus company stock rsus employee stock', u'free vacation', u'taxable benefit ability', u'free drinks snacks', u'weekly company update', u'leadership team', u'homeaway com', u'desk casual dress code'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobsy.noun_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.19854341736694686, subjectivity=0.5274019607843138)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobsy.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "career\n",
      "data\n",
      "scientist\n",
      "busi\n",
      "intellig\n",
      "216\n",
      "market\n",
      "analyt\n",
      "austin\n",
      "tx\n",
      "as\n",
      "a\n",
      "key\n",
      "member\n",
      "of\n",
      "our\n",
      "data\n",
      "scienc\n",
      "team\n",
      "you\n",
      "will\n",
      "analyz\n",
      "our\n",
      "custom\n",
      "data\n",
      "look\n",
      "for\n",
      "pattern\n",
      "and\n",
      "develop\n",
      "statist\n",
      "model\n",
      "that\n",
      "both\n",
      "support\n",
      "our\n",
      "market\n",
      "effort\n",
      "and\n",
      "improv\n",
      "our\n",
      "digit\n",
      "experi\n",
      "you\n",
      "will\n",
      "bring\n",
      "both\n",
      "scientif\n",
      "rigor\n",
      "and\n",
      "creativ\n",
      "to\n",
      "build\n",
      "robust\n",
      "and\n",
      "accur\n",
      "predict\n",
      "model\n",
      "of\n",
      "custom\n",
      "behavior\n",
      "your\n",
      "strong\n",
      "analyt\n",
      "abil\n",
      "will\n",
      "help\n",
      "us\n",
      "maxim\n",
      "the\n",
      "effect\n",
      "of\n",
      "our\n",
      "market\n",
      "effort\n",
      "and\n",
      "drive\n",
      "key\n",
      "success\n",
      "metric\n",
      "relat\n",
      "to\n",
      "yield\n",
      "manag\n",
      "and\n",
      "revenu\n",
      "gener\n",
      "you\n",
      "will\n",
      "be\n",
      "profici\n",
      "leverag\n",
      "big\n",
      "data\n",
      "to\n",
      "discov\n",
      "pattern\n",
      "use\n",
      "both\n",
      "larg\n",
      "structur\n",
      "and\n",
      "unstructur\n",
      "data\n",
      "set\n",
      "the\n",
      "ideal\n",
      "candid\n",
      "will\n",
      "have\n",
      "a\n",
      "statist\n",
      "background\n",
      "with\n",
      "train\n",
      "demonstr\n",
      "knowledg\n",
      "in\n",
      "direct\n",
      "respons\n",
      "market\n",
      "analyt\n",
      "and\n",
      "familiar\n",
      "in\n",
      "the\n",
      "principl\n",
      "of\n",
      "rfm\n",
      "in\n",
      "order\n",
      "to\n",
      "provid\n",
      "the\n",
      "analysi\n",
      "metric\n",
      "report\n",
      "that\n",
      "will\n",
      "drive\n",
      "our\n",
      "custom\n",
      "acquisit\n",
      "retent\n",
      "market\n",
      "tactic\n",
      "you\n",
      "will\n",
      "also\n",
      "engag\n",
      "in\n",
      "quantit\n",
      "analysi\n",
      "use\n",
      "comput\n",
      "intens\n",
      "data\n",
      "mine\n",
      "techniqu\n",
      "decis\n",
      "tree\n",
      "neural\n",
      "network\n",
      "cluster\n",
      "etc\n",
      "to\n",
      "deliv\n",
      "action\n",
      "insight\n",
      "that\n",
      "provid\n",
      "a\n",
      "better\n",
      "experi\n",
      "to\n",
      "our\n",
      "user\n",
      "databas\n",
      "experi\n",
      "is\n",
      "requir\n",
      "as\n",
      "you\n",
      "will\n",
      "extract\n",
      "data\n",
      "from\n",
      "variou\n",
      "sourc\n",
      "and\n",
      "with\n",
      "gener\n",
      "guidanc\n",
      "design\n",
      "construct\n",
      "complex\n",
      "mathemat\n",
      "model\n",
      "what\n",
      "our\n",
      "ideal\n",
      "candid\n",
      "look\n",
      "like\n",
      "master\n",
      "degre\n",
      "in\n",
      "appli\n",
      "mathemat\n",
      "statist\n",
      "or\n",
      "other\n",
      "quantit\n",
      "disciplin\n",
      "such\n",
      "as\n",
      "oper\n",
      "research\n",
      "or\n",
      "econom\n",
      "profici\n",
      "in\n",
      "r\n",
      "python\n",
      "sa\n",
      "or\n",
      "spss\n",
      "ms\n",
      "excel\n",
      "sql\n",
      "and\n",
      "other\n",
      "databas\n",
      "analyt\n",
      "tool\n",
      "with\n",
      "the\n",
      "abil\n",
      "to\n",
      "manipul\n",
      "extract\n",
      "analyz\n",
      "and\n",
      "report\n",
      "data\n",
      "from\n",
      "larg\n",
      "data\n",
      "warehous\n",
      "familiar\n",
      "with\n",
      "googl\n",
      "analyt\n",
      "bigqueri\n",
      "hadoop\n",
      "tableau\n",
      "web\n",
      "analyt\n",
      "a\n",
      "plu\n",
      "abil\n",
      "and\n",
      "experi\n",
      "deal\n",
      "with\n",
      "veri\n",
      "granular\n",
      "data\n",
      "prefer\n",
      "from\n",
      "a\n",
      "hadoop\n",
      "data\n",
      "storag\n",
      "platform\n",
      "except\n",
      "analyt\n",
      "skill\n",
      "with\n",
      "a\n",
      "strong\n",
      "background\n",
      "in\n",
      "predict\n",
      "model\n",
      "data\n",
      "mine\n",
      "or\n",
      "quantit\n",
      "analysi\n",
      "demonstr\n",
      "knowledg\n",
      "in\n",
      "the\n",
      "principl\n",
      "of\n",
      "linear\n",
      "and\n",
      "logist\n",
      "regress\n",
      "decis\n",
      "tree\n",
      "and\n",
      "machin\n",
      "learn\n",
      "techniqu\n",
      "work\n",
      "knowledg\n",
      "of\n",
      "the\n",
      "principl\n",
      "theori\n",
      "concept\n",
      "of\n",
      "databas\n",
      "market\n",
      "abil\n",
      "to\n",
      "work\n",
      "independ\n",
      "with\n",
      "limit\n",
      "supervis\n",
      "comfort\n",
      "with\n",
      "uncertainti\n",
      "project\n",
      "assign\n",
      "can\n",
      "chang\n",
      "rapidli\n",
      "so\n",
      "the\n",
      "abil\n",
      "to\n",
      "priorit\n",
      "is\n",
      "a\n",
      "mustbenefit\n",
      "great\n",
      "medic\n",
      "dental\n",
      "plan\n",
      "highli\n",
      "competit\n",
      "salari\n",
      "target\n",
      "annual\n",
      "bonu\n",
      "compani\n",
      "stock\n",
      "rsu\n",
      "employe\n",
      "stock\n",
      "purchas\n",
      "plan\n",
      "4\n",
      "week\n",
      "paid\n",
      "vacat\n",
      "free\n",
      "vacat\n",
      "rental\n",
      "on\n",
      "a\n",
      "yearli\n",
      "basi\n",
      "taxabl\n",
      "benefit\n",
      "abil\n",
      "to\n",
      "work\n",
      "up\n",
      "to\n",
      "two\n",
      "week\n",
      "in\n",
      "ani\n",
      "of\n",
      "our\n",
      "offic\n",
      "around\n",
      "the\n",
      "world\n",
      "on\n",
      "a\n",
      "yearli\n",
      "basi\n",
      "free\n",
      "drink\n",
      "snack\n",
      "weekli\n",
      "compani\n",
      "updat\n",
      "talk\n",
      "with\n",
      "our\n",
      "leadership\n",
      "team\n",
      "free\n",
      "list\n",
      "on\n",
      "homeaway\n",
      "com\n",
      "electr\n",
      "adjust\n",
      "stand\n",
      "up\n",
      "desk\n",
      "casual\n",
      "dress\n",
      "code\n",
      "appli\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "for word in jobsy.words:\n",
    "    print stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            216 1\n",
      "     operations 1\n",
      "           code 1\n",
      "           help 1\n",
      "        limited 1\n",
      "        storage 1\n",
      "       accurate 1\n",
      "    competitive 1\n",
      "        tableau 1\n",
      "          bring 1\n",
      "      scientist 1\n",
      "          looks 1\n",
      "    statistical 1\n",
      "          world 1\n",
      "         weekly 1\n",
      "     warehouses 1\n",
      "       networks 1\n",
      "      retention 1\n",
      "     discipline 1\n",
      "            web 1\n",
      "        rapidly 1\n",
      "      knowledge 3\n",
      "             tx 1\n",
      "       guidance 1\n",
      "       homeaway 1\n",
      "      intensive 1\n",
      "             to 11\n",
      "          other 2\n",
      "              4 1\n",
      "         annual 1\n",
      "       employee 1\n",
      "       granular 1\n",
      "        careers 1\n",
      "        applied 1\n",
      "         around 1\n",
      "    exceptional 1\n",
      "         python 1\n",
      "            big 1\n",
      "          trees 2\n",
      "   unstructured 1\n",
      "     background 2\n",
      "           desk 1\n",
      "         report 1\n",
      "          using 2\n",
      "         rental 1\n",
      "       projects 1\n",
      "        masters 1\n",
      "           like 1\n",
      "        success 1\n",
      "        sources 1\n",
      "         skills 1\n",
      "          yield 1\n",
      "          large 2\n",
      "    assignments 1\n",
      "        benefit 1\n",
      "        offices 1\n",
      "       logistic 1\n",
      "           team 2\n",
      "     analytical 2\n",
      "            rfm 1\n",
      "   mathematical 1\n",
      "     predictive 2\n",
      "         google 1\n",
      "       learning 1\n",
      "     generation 1\n",
      "   intelligence 1\n",
      "       discover 1\n",
      "        related 1\n",
      "      analytics 5\n",
      "       computer 1\n",
      "         design 1\n",
      "            our 10\n",
      "        extract 2\n",
      "         casual 1\n",
      "     scientific 1\n",
      "     principles 3\n",
      "           what 1\n",
      "         degree 1\n",
      "            for 1\n",
      "       decision 2\n",
      "     proficient 1\n",
      "    familiarity 2\n",
      "       research 1\n",
      "        looking 1\n",
      "          bonus 1\n",
      "             ms 1\n",
      "        machine 1\n",
      "        various 1\n",
      "            com 1\n",
      "       electric 1\n",
      "   mustbenefits 1\n",
      "         dental 1\n",
      "   demonstrated 2\n",
      "             be 1\n",
      "        ability 6\n",
      "       business 1\n",
      "      marketing 6\n",
      "     leveraging 1\n",
      "        deliver 1\n",
      "           free 3\n",
      "     statistics 2\n",
      "           paid 1\n",
      "            key 2\n",
      "       modeling 1\n",
      "         strong 2\n",
      "         change 1\n",
      "        improve 1\n",
      "           both 3\n",
      "       training 1\n",
      "         engage 1\n",
      "      candidate 2\n",
      "             of 8\n",
      "      economics 1\n",
      "     experience 4\n",
      "           spss 1\n",
      "            etc 1\n",
      "     leadership 1\n",
      "           plus 1\n",
      "          stand 1\n",
      "           rsus 1\n",
      "        efforts 2\n",
      "         austin 1\n",
      "             or 4\n",
      "         yearly 2\n",
      "            two 1\n",
      "     adjustable 1\n",
      "        revenue 1\n",
      "         highly 1\n",
      "       maximize 1\n",
      "           such 1\n",
      "          apply 1\n",
      "          tools 1\n",
      "           your 1\n",
      "     manipulate 1\n",
      "     clustering 1\n",
      "         better 1\n",
      "     management 1\n",
      "           from 3\n",
      "        working 1\n",
      "       platform 1\n",
      "        support 1\n",
      "     structured 1\n",
      "     creativity 1\n",
      "       vacation 2\n",
      "      construct 1\n",
      "         linear 1\n",
      "        dealing 1\n",
      "        analyze 2\n",
      "         salary 1\n",
      "         mining 2\n",
      "           that 3\n",
      "        company 2\n",
      "        listing 1\n",
      "          great 1\n",
      "         direct 1\n",
      "    proficiency 1\n",
      "          talks 1\n",
      "          basis 2\n",
      "        taxable 1\n",
      "    comfortable 1\n",
      "           with 9\n",
      "          weeks 2\n",
      "       insights 1\n",
      "       customer 3\n",
      "        tactics 1\n",
      "         target 1\n",
      "     developing 1\n",
      "        science 1\n",
      "       database 3\n",
      "             up 2\n",
      "             us 1\n",
      "       theories 1\n",
      "           will 8\n",
      "              r 1\n",
      "            can 1\n",
      "       behavior 1\n",
      "       concepts 1\n",
      "    acquisition 1\n",
      "            and 14\n",
      "    uncertainty 1\n",
      "         snacks 1\n",
      "             is 2\n",
      "        general 1\n",
      "             as 3\n",
      "    supervision 1\n",
      "           have 1\n",
      "             in 9\n",
      "            sas 1\n",
      "           work 2\n",
      "          dress 1\n",
      "     regression 1\n",
      "     preferably 1\n",
      "        provide 2\n",
      "          plans 1\n",
      "          excel 1\n",
      "         hadoop 2\n",
      "            any 1\n",
      "         member 1\n",
      "           also 1\n",
      "          ideal 2\n",
      "        complex 1\n",
      "          build 1\n",
      "        digital 1\n",
      "    mathematics 1\n",
      "            you 5\n",
      "  effectiveness 1\n",
      "          stock 2\n",
      "          users 1\n",
      "         models 3\n",
      "      reporting 1\n",
      "     techniques 2\n",
      "         update 1\n",
      "       bigquery 1\n",
      "        metrics 2\n",
      "          rigor 1\n",
      "           plan 1\n",
      "            sql 1\n",
      "         robust 1\n",
      "             on 3\n",
      "           data 12\n",
      "       response 1\n",
      "              a 9\n",
      "       purchase 1\n",
      "  independently 1\n",
      "         neural 1\n",
      "     prioritize 1\n",
      "        medical 1\n",
      "       required 1\n",
      "         drinks 1\n",
      "          drive 2\n",
      "       analysis 3\n",
      "     actionable 1\n",
      "       patterns 2\n",
      "             so 1\n",
      "           sets 1\n",
      "           very 1\n",
      "   quantitative 3\n",
      "            the 9\n",
      "          order 1\n"
     ]
    }
   ],
   "source": [
    "for word, count in jobsy.word_counts.items():\n",
    "    print \"%15s %i\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'How are you, dear Mr. Sir?', 'Are you well?', 'Here: drink this!', 'It will make you feel better.', \"I mean, it won't make you feel worse!\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Hello. How are you, dear Mr. Sir? Are you well?\n",
    "          Here: drink this! It will make you feel better.\n",
    "          I mean, it won't make you feel worse!\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/patrick/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/patrick/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting gensim\n",
      "  Downloading gensim-0.12.4.tar.gz (2.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.4MB 495kB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): numpy>=1.3 in /home/patrick/anaconda2/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.7.0 in /home/patrick/anaconda2/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /home/patrick/anaconda2/lib/python2.7/site-packages (from gensim)\n",
      "Collecting smart_open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.3.3.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): boto>=2.32 in /home/patrick/anaconda2/lib/python2.7/site-packages (from smart_open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart_open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /home/patrick/anaconda2/lib/python2.7/site-packages (from smart_open>=1.2.1->gensim)\n",
      "Installing collected packages: bz2file, smart-open, gensim\n",
      "  Running setup.py install for bz2file ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Running setup.py install for smart-open ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Running setup.py install for gensim ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25hSuccessfully installed bz2file-0.98 gensim-0.12.4 smart-open-1.3.3\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_list = []\n",
    "for post in coll.find():\n",
    "    job_list.append(post.values()[2])\n",
    "#coll.find_one().values()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasetsd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set categories\n",
    "categories = ['start-up', 'corporation']#, \n",
    "              #'rec.motorcycles', 'sci.space', 'talk.politics.mideast']\n",
    "# Download the training subset of the 20 NG dataset, with headers, footers, quotes removed\n",
    "# Only keep docs from the 6 categories above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.1, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 2), stop_words='english',\n",
    "                                  token_pattern='\\\\b[a-z][a-z]+\\\\b', min_df=2, max_df = 0.10)\n",
    "count_vectorizer.fit(job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3286, 36)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_vecs = count_vectorizer.transform(job_list).transpose()\n",
    "job_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csc.csc_matrix"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(job_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(job_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.matutils.Sparse2Corpus"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3286"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus, id2word=id2word, num_topics=2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.009*icon + 0.007*datameer + 0.002*form + 0.002*assembly + 0.002*general assembly'),\n",
       " (1,\n",
       "  u'0.014*attribution + 0.003*leading digital + 0.002*pricing + 0.002*device + 0.002*retailer brand')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=5, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in job_list:\n",
    "    for word in item.split():\n",
    "        if word == 'retailmenot':\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'retailmenot  inc  career opportunities data scientist please enable cookies to continue please enable cookies in your browser to experience all the personalized features of this site  including the ability to apply for a job  returning candidate  log back in  data scientist data scientist category engineering job location us tx austin more information about this job  about the role  we are looking for exceptional talent to help drive innovation within our applied data science team  if you are passionate about data science  machine learning and predictive algorithms  then this is a unique opportunity to see your ideas propagate to millions of users our team and our workspace our engineering team is built from world class experts in internet and mobile technologies  we really take pride of our fast paced and fun environment in which we constantly learn from each other to develop our skills  our workspace  which is located in the heart of austin  tx  is stunningly modern and open  austin is a fast growing hub for tech startups and larger companies  besides technology  austin is full of culture  music  food  and outdoor activities responsibilities use data mining and machine learning to design and develop recommendation engines and other data products which drive user engagement  retention and monetization design and implement scalable algorithms and models develop and code softwareprograms  algorithms and automated processes to cleanse  integrate and evaluate large datasets in order to gain actionable insights communicate effectively to different audiences  both technical and non technical  through story telling and data visualization collaborate effectively with engineers  data scientists  product managers and other internal customers keep abreast of new technologies  technologies we use today include  python  hadoop  hive  spark  redshift  and elasticsearch qualifications strong analytical background ms phd degree in computer science  statistics  applied mathematics or related quantitative field or equivalent self study 2+ years experience working in collaboration with software engineering teams to productize your work specific experience in multiple machine learning and inferencing techniques such as regression  svms  matrix factorization  factorization machines  boosting   ensemble methods  hmms   graphical models  deep learning  markov decision processes  and clustering techniques experience building scalable algorithms in python  java or other languages good familiarity with key elements of the pydata ecosystem such as numpy  pandas  scikit learn familiarity with a number of elements of the hadoop ecosystem including hdfs  hive  mapreduce  and spark familiarity with classical statistics probability and time series analysis experience querying relational databases  sql  and independently working with and exploring data a pragmatic approach to problem solving  establishing conceptual connections to data sources  understanding relationships among data  and comfort with uncertainties approximations great communication and teamwork skills  about us  retailmenot  inc   http   www retailmenot com corp  is a leadingdigital savings destination connecting consumers with retailers  restaurantsand brands  both online and in store the company enables consumers across the globe to find hundreds of thousands of digital offers to save money while they shop or dine out  during the 12 months ended december 31  2015  retailmenot  inc  experienced over 718million visits to its websites  it also averaged 23 2 million mobile unique visitors per month during the three months ended december 31  2015 we estimate that approximately  4 8 billion in retailer sales we re attributable to consumer transactions from paid digital offers in our marketplace in 2015  more than  600million of which we re attributable to our in store solution  the retailmenot  inc  portfolio of websites and mobile applications includesretailmenot com in the united states  retailmenot ca in canada vouchercodes co uk in the united kingdom retailmenot dein germany  actiepagina nl in the netherlands ma reduc comandpoulpeo com in france  anddeals2buy comin north america  retailmenot  inc  is listed on the nasdaq stock exchange underthe ticker symbol  sale   investors interested in learning more about the company can visithttp   investor retailmenot com  rewards  we offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition  benefits of being an employee of retailmenot  inc  include  but are not limited to the following competitive base   bonus packagesequity grantsperformance based rewards   recognition for your hard work and servicevery competitive benefits packagesopen   flexible ptocell phone   gym membership reimbursementsfully stocked break room   onsite catered breakfasts   lunches multiple days weeku s  equal employment opportunity affirmative action informationindividuals seeking employment at retailmenot  inc  are considered without regards to race  color  religion  national origin  age  sex  marital status  ancestry  physical or mental disability  veteran status  or sexual orientation  you are being given the opportunity to provide the following information in order to help us comply with federal and state equal employment opportunity affirmative action record keeping  reporting  and other legal requirements  options  apply for this job onlineapplysharerefer a friend to this jobrefer share on your newsfeed go back to the welcome page application faqssoftware powe red by icimswww icims com'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
